# MLP-GAN Training Notebook
# Implementaci√≥n y entrenamiento de GAN cl√°sica en Fashion-MNIST

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
from tqdm import tqdm

# Configuraci√≥n
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Dispositivo: {device}")

# Hiperpar√°metros
BATCH_SIZE = 64
LEARNING_RATE = 0.0002
NUM_EPOCHS = 100
Z_DIM = 100
IMAGE_SIZE = 28
BETA1 = 0.5

# Reproducibilidad
torch.manual_seed(42)
np.random.seed(42)


class MLPGenerator(nn.Module):
    """
    Generador MLP para Fashion-MNIST
    Convierte vector de ruido z en imagen 28x28
    """

    def __init__(self, z_dim=100, img_dim=784):
        super(MLPGenerator, self).__init__()
        self.img_dim = img_dim

        self.model = nn.Sequential(
            # Primera capa: expandir desde z_dim a 256
            nn.Linear(z_dim, 256),
            nn.LeakyReLU(0.2, inplace=True),

            # Segunda capa: 256 -> 512
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2, inplace=True),

            # Tercera capa: 512 -> 1024
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2, inplace=True),

            # Capa de salida: 1024 -> img_dim
            nn.Linear(1024, img_dim),
            nn.Tanh()  # Salida en rango [-1, 1]
        )

    def forward(self, z):
        """
        Forward pass del generador
        z: tensor de ruido (batch_size, z_dim)
        return: tensor de imagen (batch_size, img_dim)
        """
        return self.model(z)


class MLPDiscriminator(nn.Module):
    """
    Discriminador MLP para Fashion-MNIST
    Clasifica si una imagen es real o generada
    """

    def __init__(self, img_dim=784):
        super(MLPDiscriminator, self).__init__()

        self.model = nn.Sequential(
            # Primera capa con dropout
            nn.Linear(img_dim, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),

            # Segunda capa
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),

            # Tercera capa
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),

            # Capa de salida
            nn.Linear(256, 1),
            nn.Sigmoid()  # Probabilidad de ser real
        )

    def forward(self, img):
        """
        Forward pass del discriminador
        img: tensor de imagen aplanada (batch_size, img_dim)
        return: probabilidad de ser real (batch_size, 1)
        """
        return self.model(img)


def weights_init(m):
    """Inicializaci√≥n de pesos mejorada para ambos tipos de red"""
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        # Para capas lineales, usar inicializaci√≥n Xavier
        nn.init.xavier_normal_(m.weight.data, gain=0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias.data, 0)
    elif classname.find('Conv') != -1:
        # Para futuras extensiones con capas convolucionales
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)


def load_fashion_mnist():
    """Carga el dataset Fashion-MNIST"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))  # Normalizar a [-1, 1]
    ])

    dataset = torchvision.datasets.FashionMNIST(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2
    )

    return dataloader


def train_mlp_gan():
    """Funci√≥n principal de entrenamiento con mejoras en estabilidad"""
    print("=== Entrenando MLP-GAN con Mejoras ===")

    # Cargar datos
    dataloader = load_fashion_mnist()
    print(f"Dataset cargado: {len(dataloader.dataset)} im√°genes")

    # Crear modelos
    generator = MLPGenerator(Z_DIM, IMAGE_SIZE * IMAGE_SIZE).to(device)
    discriminator = MLPDiscriminator(IMAGE_SIZE * IMAGE_SIZE).to(device)

    # Aplicar inicializaci√≥n mejorada
    print("Aplicando inicializaci√≥n de pesos...")
    generator.apply(weights_init)
    discriminator.apply(weights_init)

    print(f"Generador - Par√°metros: {sum(p.numel() for p in generator.parameters()):,}")
    print(f"Discriminador - Par√°metros: {sum(p.numel() for p in discriminator.parameters()):,}")

    # Optimizadores
    opt_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))
    opt_disc = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))

    # Funci√≥n de p√©rdida
    criterion = nn.BCELoss()

    # Tracking de p√©rdidas
    gen_losses = []
    disc_losses = []

    # Vector de ruido fijo para visualizaci√≥n
    fixed_noise = torch.randn(16, Z_DIM, device=device)

    print("Iniciando entrenamiento...")

    for epoch in range(NUM_EPOCHS):
        gen_loss_epoch = 0
        disc_loss_epoch = 0
        discriminator_too_strong = 0  # Contador para diagn√≥stico

        # Barra de progreso
        pbar = tqdm(dataloader, desc=f'√âpoca {epoch + 1}/{NUM_EPOCHS}')

        for batch_idx, (real_imgs, _) in enumerate(pbar):
            batch_size = real_imgs.shape[0]
            real_imgs = real_imgs.view(batch_size, -1).to(device)

            # Etiquetas
            real_labels = torch.ones(batch_size, 1, device=device)
            fake_labels = torch.zeros(batch_size, 1, device=device)

            # ===============================
            # Entrenar Discriminador
            # ===============================
            discriminator.zero_grad()

            # P√©rdida con im√°genes reales
            real_pred = discriminator(real_imgs)
            real_loss = criterion(real_pred, real_labels)

            # Generar im√°genes falsas
            noise = torch.randn(batch_size, Z_DIM, device=device)
            fake_imgs = generator(noise)

            # P√©rdida con im√°genes falsas
            fake_pred = discriminator(fake_imgs.detach())
            fake_loss = criterion(fake_pred, fake_labels)

            # P√©rdida total del discriminador
            disc_loss = real_loss + fake_loss

            # MEJORA: Evitar entrenamiento excesivo del discriminador
            if disc_loss.item() > 0.1:  # Solo entrenar si no es demasiado fuerte
                disc_loss.backward()
                opt_disc.step()
            else:
                discriminator_too_strong += 1

            # ===============================
            # Entrenar Generador
            # ===============================
            generator.zero_grad()

            # El generador quiere que el discriminador clasifique
            # las im√°genes falsas como reales
            fake_pred = discriminator(fake_imgs)
            gen_loss = criterion(fake_pred, real_labels)

            gen_loss.backward()
            opt_gen.step()

            # Actualizar m√©tricas
            gen_loss_epoch += gen_loss.item()
            disc_loss_epoch += disc_loss.item()

            # Actualizar barra de progreso
            pbar.set_postfix({
                'D_loss': f'{disc_loss.item():.4f}',
                'G_loss': f'{gen_loss.item():.4f}',
                'D_strong': discriminator_too_strong
            })

        # Promediar p√©rdidas de la √©poca
        gen_loss_epoch /= len(dataloader)
        disc_loss_epoch /= len(dataloader)

        gen_losses.append(gen_loss_epoch)
        disc_losses.append(disc_loss_epoch)

        # Imprimir estad√≠sticas cada 10 √©pocas
        if (epoch + 1) % 10 == 0:
            print(f'√âpoca [{epoch + 1}/{NUM_EPOCHS}]')
            print(f'  P√©rdida Discriminador: {disc_loss_epoch:.4f}')
            print(f'  P√©rdida Generador: {gen_loss_epoch:.4f}')
            print(f'  Discriminador demasiado fuerte: {discriminator_too_strong}/{len(dataloader)} batches')

            # An√°lisis de equilibrio
            loss_ratio = gen_loss_epoch / max(disc_loss_epoch, 1e-8)
            if loss_ratio > 5:
                print(f'  ‚ö†Ô∏è Discriminador dominando (ratio G/D: {loss_ratio:.2f})')
            elif loss_ratio < 0.2:
                print(f'  ‚ö†Ô∏è Generador dominando (ratio G/D: {loss_ratio:.2f})')
            else:
                print(f'  ‚úÖ Entrenamiento balanceado (ratio G/D: {loss_ratio:.2f})')

            # Generar y mostrar im√°genes de muestra
            with torch.no_grad():
                fake_sample = generator(fixed_noise)
                fake_sample = fake_sample.view(-1, 1, 28, 28)
                show_sample_images(fake_sample, epoch + 1)

    return generator, discriminator, gen_losses, disc_losses


def show_sample_images(images, epoch):
    """Muestra una grilla de im√°genes generadas"""
    # Desnormalizar de [-1, 1] a [0, 1]
    images = (images + 1) / 2

    fig, axes = plt.subplots(4, 4, figsize=(8, 8))
    fig.suptitle(f'MLP-GAN - Im√°genes Generadas - √âpoca {epoch}', fontsize=14)

    for i in range(16):
        row = i // 4
        col = i % 4
        axes[row, col].imshow(images[i].cpu().squeeze(), cmap='gray')
        axes[row, col].axis('off')

    plt.tight_layout()
    plt.show()


def plot_training_curves(gen_losses, disc_losses):
    """Grafica las curvas de entrenamiento con an√°lisis mejorado"""
    plt.figure(figsize=(15, 5))

    # P√©rdidas
    plt.subplot(1, 3, 1)
    plt.plot(gen_losses, label='Generador', color='blue', alpha=0.7)
    plt.plot(disc_losses, label='Discriminador', color='red', alpha=0.7)
    plt.xlabel('√âpocas')
    plt.ylabel('P√©rdida')
    plt.title('Curvas de P√©rdida - MLP-GAN')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Suavizado con media m√≥vil
    plt.subplot(1, 3, 2)
    window = 5
    if len(gen_losses) >= window:
        gen_smooth = np.convolve(gen_losses, np.ones(window) / window, mode='valid')
        disc_smooth = np.convolve(disc_losses, np.ones(window) / window, mode='valid')

        plt.plot(gen_smooth, label='Generador (suavizado)', color='blue')
        plt.plot(disc_smooth, label='Discriminador (suavizado)', color='red')
        plt.xlabel('√âpocas')
        plt.ylabel('P√©rdida')
        plt.title('Curvas Suavizadas')
        plt.legend()
        plt.grid(True, alpha=0.3)

    # Ratio de p√©rdidas (indicador de equilibrio)
    plt.subplot(1, 3, 3)
    ratios = [g / max(d, 1e-8) for g, d in zip(gen_losses, disc_losses)]
    plt.plot(ratios, color='green', alpha=0.7)
    plt.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Equilibrio ideal')
    plt.xlabel('√âpocas')
    plt.ylabel('Ratio G/D')
    plt.title('Equilibrio de Entrenamiento')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.ylim(0, max(10, max(ratios) * 1.1))

    plt.tight_layout()
    plt.show()


def evaluate_model(generator):
    """Eval√∫a el modelo generador entrenado con m√©tricas mejoradas"""
    generator.eval()

    print("=== Evaluaci√≥n del Modelo MLP-GAN ===")

    # Generar batch de im√°genes
    with torch.no_grad():
        noise = torch.randn(64, Z_DIM, device=device)
        fake_imgs = generator(noise)
        fake_imgs = fake_imgs.view(-1, 1, 28, 28)
        fake_imgs = (fake_imgs + 1) / 2  # Desnormalizar

    # Mostrar galer√≠a de im√°genes generadas
    fig, axes = plt.subplots(8, 8, figsize=(12, 12))
    fig.suptitle('Galer√≠a de Im√°genes Generadas - MLP-GAN', fontsize=16)

    for i in range(64):
        row = i // 8
        col = i % 8
        axes[row, col].imshow(fake_imgs[i].cpu().squeeze(), cmap='gray')
        axes[row, col].axis('off')

    plt.tight_layout()
    plt.show()

    # Estad√≠sticas detalladas
    pixel_mean = fake_imgs.mean().item()
    pixel_std = fake_imgs.std().item()
    pixel_min = fake_imgs.min().item()
    pixel_max = fake_imgs.max().item()

    print(f"Estad√≠sticas de p√≠xeles generados:")
    print(f"  Media: {pixel_mean:.4f}")
    print(f"  Desviaci√≥n est√°ndar: {pixel_std:.4f}")
    print(f"  Rango: [{pixel_min:.4f}, {pixel_max:.4f}]")

    # An√°lisis de calidad
    if pixel_std < 0.1:
        print(f"  ‚ö†Ô∏è Advertencia: Baja variaci√≥n en p√≠xeles (posible colapso)")
    elif pixel_std > 0.4:
        print(f"  ‚ö†Ô∏è Advertencia: Alta variaci√≥n en p√≠xeles (posible ruido)")
    else:
        print(f"  ‚úÖ Variaci√≥n de p√≠xeles en rango saludable")

    # An√°lisis de diversidad simple
    img_flat = fake_imgs.view(64, -1)
    distances = torch.cdist(img_flat, img_flat)
    mean_distance = distances.mean().item()

    print(f"  Diversidad promedio: {mean_distance:.4f}")
    if mean_distance < 0.1:
        print(f"  ‚ö†Ô∏è Advertencia: Baja diversidad (posible mode collapse)")
    else:
        print(f"  ‚úÖ Diversidad aceptable")


def save_training_results(generator, discriminator, gen_losses, disc_losses):
    """Guarda los resultados del entrenamiento"""
    # Crear directorio si no existe
    os.makedirs('results/MLP-GAN', exist_ok=True)

    # Guardar modelos
    torch.save(generator.state_dict(), 'results/MLP-GAN/mlp_generator.pth')
    torch.save(discriminator.state_dict(), 'results/MLP-GAN/mlp_discriminator.pth')

    # Guardar p√©rdidas
    import json
    losses_data = {
        'generator_losses': gen_losses,
        'discriminator_losses': disc_losses,
        'hyperparameters': {
            'batch_size': BATCH_SIZE,
            'learning_rate': LEARNING_RATE,
            'num_epochs': NUM_EPOCHS,
            'z_dim': Z_DIM,
            'beta1': BETA1
        }
    }

    with open('results/MLP-GAN/training_losses.json', 'w') as f:
        json.dump(losses_data, f, indent=2)

    print("üíæ Resultados guardados en results/MLP-GAN/")


def main():
    """Funci√≥n principal del notebook"""
    print("üöÄ Iniciando entrenamiento MLP-GAN mejorado en Fashion-MNIST")
    print(f"Configuraci√≥n:")
    print(f"  - Dispositivo: {device}")
    print(f"  - Batch size: {BATCH_SIZE}")
    print(f"  - Learning rate: {LEARNING_RATE}")
    print(f"  - √âpocas: {NUM_EPOCHS}")
    print(f"  - Dimensi√≥n z: {Z_DIM}")

    # Entrenar modelo
    generator, discriminator, gen_losses, disc_losses = train_mlp_gan()

    # Visualizar curvas de entrenamiento
    plot_training_curves(gen_losses, disc_losses)

    # Evaluar modelo final
    evaluate_model(generator)

    # Guardar resultados
    save_training_results(generator, discriminator, gen_losses, disc_losses)

    # An√°lisis final
    print("\n" + "=" * 50)
    print("AN√ÅLISIS FINAL DEL ENTRENAMIENTO")
    print("=" * 50)

    final_g_loss = gen_losses[-1]
    final_d_loss = disc_losses[-1]
    final_ratio = final_g_loss / max(final_d_loss, 1e-8)

    print(f"P√©rdida final del Generador: {final_g_loss:.4f}")
    print(f"P√©rdida final del Discriminador: {final_d_loss:.4f}")
    print(f"Ratio final G/D: {final_ratio:.4f}")

    # Estabilidad en las √∫ltimas √©pocas
    last_10_g = gen_losses[-10:] if len(gen_losses) >= 10 else gen_losses
    last_10_d = disc_losses[-10:] if len(disc_losses) >= 10 else disc_losses

    g_stability = np.std(last_10_g)
    d_stability = np.std(last_10_d)

    print(f"Estabilidad del Generador (√∫ltimas √©pocas): {g_stability:.4f}")
    print(f"Estabilidad del Discriminador (√∫ltimas √©pocas): {d_stability:.4f}")

    if g_stability < 0.1 and d_stability < 0.1:
        print("‚úÖ Entrenamiento estable conseguido")
    else:
        print("‚ö†Ô∏è Entrenamiento todav√≠a muestra inestabilidad")

    print("‚úÖ Entrenamiento completado!")
    print("üìÅ Modelos guardados como 'results/MLP-GAN/mlp_generator.pth' y 'mlp_discriminator.pth'")


# Ejecutar solo si es el script principal
if __name__ == "__main__":
    main()